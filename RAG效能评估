核心在于衡量其利用外部知识生成可靠响应的能力

1.查询-上下文相关性评估

ARES、RAGAS和TruLens在内的多个框架采用上下文相关性指标来衡量检索到的上下文与用户查询的对齐与支持程度。
专为知识密集型任务设计的KILT基准测试提供了R-precision和Recall@k等指标来评估检索上下文的质量，强调其与原始查询的相关性和完整性。


2.上下文-答案一致性评估

CRUD采用召回率和精确率来评估生成答案的准确性，重点关注响应的正确性与完整性。
ARES与RAGAS引入了答案忠实度指标，用于衡量生成答案对检索段落的忠实程度，确保内容无幻觉。
RECALL提出了两个关键指标：准确率用于评估生成答案的事实正确性，误导率则衡量产生误导信息的频率。
此外，TruLens引入了事实依据性概念，其将LLM生成的答案分解为单个主张，并对照检索上下文逐一验证，确保输出有具体证据支持。

除了这些核心指标，近期研究还通过高质量引用与准确归因来提升RAG系统的可靠性。

LongBench-Cite专为评估长上下文引用问答中的模型而设计，其中引用的相关性与准确性对于保持透明度至关重要。

RECLAIM框架进一步发展了这一理念，提出正确归因分数用于评估生成答案是否完全得到引用支持，以及引用冗余分数用于减少不必要的引用以提升回答清晰度。
RECLAIM中的可验证性和一致性比率等指标确保检索数据真正支撑生成内容。

此外，RARR引入句子级AIS和Levenshtein距离来衡量引用准确性，确保参考文献保留原意。

WebGPT通过区分模仿性错误与非模仿性错误以改进纠错，增加了一层可靠性。

ALCE则使用引用召回率和精确率来评估引用的相关性与简洁性。

这些指标共同构成了一个全面的评估框架，用于衡量RAG生成基于上下文、准确且归因透明的响应的能力，强调了实际应用中的可靠性与可信度。

3.查询-答案准确性评估
在RAG系统中，准确衡量用户查询与生成答案之间的对齐对于评估整体性能至关重要。

RAG评估中的一个重大挑战是处理幻觉问题——即模型生成的内容缺乏检索上下文依据的情况。

为解决此问题，CRAG等基准测试将幻觉率和缺失率与传统准确率结合，通过计算准确率与幻觉率之差来惩罚幻觉，从而优先考虑事实正确的回答。

其他框架如RAGAS、ARES和TruLens包含了答案相关性指标，用于评估生成响应与原始查询的对齐程度。

类似地，CRUD引入RAGQuestEval来评估答案的相关性与正确性，而KILT基准测试则基于特定下游任务评估RAG系统，为生成答案提供任务导向的视角。

此外，RGB框架通过引入拒绝率、错误检测率和错误纠正率扩展了评估标准，这些指标测试了系统识别并拒绝错误或误导信息的能力。
