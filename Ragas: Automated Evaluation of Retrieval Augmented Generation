Ragas: Automated Evaluation of Retrieval Augmented Generation
https://arxiv.org/pdf/2309.15217

1.摘要
我们提出了 Ragas（检索增强生成评估）框架，这是一种用于检索增强生成（RAG）流水线的无参考评估方法。

检索增强生成系统由检索模块和基于大语言模型（LLM）的生成模块组成，可从参考文本数据库中为大语言模型提供知识，
使其能够充当用户与文本数据库之间的自然语言交互层，从而降低幻觉风险。

然而，评估检索增强生成架构具有一定挑战性，原因在于需要考量多个维度：
检索系统识别相关且聚焦的上下文段落的能力、
大语言模型以可靠方式利用这些段落的能力，
以及生成内容本身的质量。

借助 Ragas 框架，我们提出了一套评估指标，可在不依赖人工标注真值的情况下，对这些不同维度进行评估。
我们认为，该框架能够为加快检索增强生成架构的评估周期起到关键作用，这一点在大语言模型被快速采用的背景下尤为重要。

2.结论
本文强调了对检索增强生成（RAG）系统进行自动化无参考评估的必要性。

具体而言，我们论证了构建一个评估框架的需求，该框架需能够对以下三个方面进行评估：
忠实度（即答案是否基于检索到的上下文）、
答案相关性（即答案是否回应了问题）
以及上下文相关性（即检索到的上下文是否具有足够的聚焦性）。

为支持该框架的开发，我们引入了 WikiEval 数据集，该数据集包含人类对上述三个不同方面的标注结果。

最后，我们还介绍了 Ragas—— 我们针对上述三个质量维度所实现的评估工具。
该框架易于使用，即便在缺乏任何真值标注的情况下，也能为检索增强生成系统的开发者提供有价值的参考信息。

我们在 WikiEval 数据集上的评估结果表明，Ragas 的预测结果与人类的判断高度吻合，在忠实度和答案相关性这两个维度上表现尤为突出。
（注：原文中 “deverlopers” 为拼写错误，已在译文中修正为 “开发者”；“a dataset which human judgements” 存在语法缺失，结合上下文补充为 “该数据集包含人类对上述三个不同方面的标注结果”，以保证语义完整。）

3.评估策略
我们采用标准的检索增强生成（RAG）场景：

给定问题q，系统首先检索相关上下文c(q)，随后利用检索到的上下文生成答案as(q)。

在构建检索增强生成系统时，我们通常无法获取人工标注的数据集或参考答案。
因此，我们将研究重点放在完全自包含且无参考的评估指标上。
具体而言，我们聚焦于三个核心质量维度，我们认为这三个维度至关重要。

第一，忠实度（Faithfulness），指答案必须基于给定的上下文。
这一维度对于避免生成幻觉、确保检索到的上下文可作为生成答案的依据至关重要。
事实上，检索增强生成系统常被应用于对生成文本与参考来源的事实一致性要求极高的场景，
例如信息持续更新的法律领域。

第二，答案相关性（Answer Relevance），指生成的答案需准确回应所提出的问题。

第三，上下文相关性（Context Relevance），指检索到的上下文需具备聚焦性，尽可能减少无关信息。
考虑到向大语言模型（LLM）输入长上下文存在成本问题，
同时过长的上下文会降低大语言模型对其的利用效率 —— 尤其是对上下文中间部分信息的利用（Liu 等人，2023），这一维度尤为重要。

下文将阐述如何通过提示大语言模型，以完全自动化的方式对这三个质量维度进行度量。

在本研究的实现与实验过程中，所有提示均通过 OpenAI API 调用 gpt-3.5-turbo-16k 模型进行评估。

忠实度若答案中的所有主张均可从上下文中推断得出，则称答案as(q)对上下文c(q)具有忠实度。
为评估忠实度，我们首先利用大语言模型从答案中提取一组陈述S(as(q))。
此步骤的目的是将较长的句子分解为更简短、聚焦的断言。
该步骤使用的提示语如下：给定问题与答案，请从答案的每个句子中提炼一个或多个陈述。
问题：[问题内容]
答案：[答案内容]
其中，[问题内容] 和 [答案内容] 分别替换为具体的问题与答案。

对于集合S中的每个陈述s_i，大语言模型通过验证函数v(s_i, c(q))判断该陈述是否可从上下文c(q)中推断得出。
验证步骤使用的提示语如下：请结合给定上下文与下列陈述，判断这些陈述是否得到上下文中信息的支持。
在得出最终结论（是 / 否）前，请为每个陈述提供简要解释。

最后，请按顺序以指定格式给出每个陈述的最终结论，不得偏离该格式。
陈述：[陈述 1]
......
陈述：[陈述 n]

最终的忠实度得分F计算公式为：F=V/S，其中V为大语言模型判定为得到支持的陈述数量，S为陈述的总数量。

答案相关性
若答案以恰当的方式直接回应问题，则称答案a_s(q)具有相关性。
需要说明的是，我们对答案相关性的评估不考虑事实准确性，但会对答案存在遗漏或包含冗余信息的情况进行惩罚。
为评估答案相关性，针对给定答案a_s(q)，我们提示大语言模型基于该答案生成n个潜在问题q_i，使用的提示语如下：
请为给定答案生成一个对应的问题。
答案：[答案内容]

随后，我们通过 OpenAI API 调用 text-embedding-ada-002 模型，获取所有问题的嵌入向量。
对于每个潜在问题q_i，我们计算其与原始问题q的相似度sim(q, q_i)，即对应嵌入向量之间的余弦相似度。
问题q对应的答案相关性得分AR计算公式为：
$$\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)$$
\(AR=\frac{1}{n}\sum_{i=1}^{n}\text{sim}(q, q_i) \quad (1)\)该指标用于评估生成答案与初始问题或指令的契合程度。上下文相关性若上下文仅包含回答问题所需的信息，则称上下文\(c(q)\)具有相关性。该指标旨在对包含冗余信息的情况进行惩罚。为评估上下文相关性，给定问题q及其上下文\(c(q)\)，我们提示大语言模型从上下文中提取对回答问题至关重要的句子子集\(S_{ext}\)，使用的提示语如下：请从提供的上下文中提取可能有助于回答下列问题的相关句子。若未找到相关句子，或你认为无法从给定上下文中回答该问题，请返回短语 “信息不足”。提取候选句子时，不得对上下文中的句子进行任何修改。上下文相关性得分的计算公式为：\(CR=\frac{\text{提取的句子数量}}{\text{上下文中的总句子数量}} \quad (2)\)
