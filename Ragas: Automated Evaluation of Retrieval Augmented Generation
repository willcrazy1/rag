Ragas: Automated Evaluation of Retrieval Augmented Generation
https://arxiv.org/pdf/2309.15217

1.摘要
我们提出了 Ragas（检索增强生成评估）框架，这是一种用于检索增强生成（RAG）流水线的无参考评估方法。

检索增强生成系统由检索模块和基于大语言模型（LLM）的生成模块组成，可从参考文本数据库中为大语言模型提供知识，
使其能够充当用户与文本数据库之间的自然语言交互层，从而降低幻觉风险。

然而，评估检索增强生成架构具有一定挑战性，原因在于需要考量多个维度：
检索系统识别相关且聚焦的上下文段落的能力、
大语言模型以可靠方式利用这些段落的能力，
以及生成内容本身的质量。

借助 Ragas 框架，我们提出了一套评估指标，可在不依赖人工标注真值的情况下，对这些不同维度进行评估。
我们认为，该框架能够为加快检索增强生成架构的评估周期起到关键作用，这一点在大语言模型被快速采用的背景下尤为重要。

2.结论
本文强调了对检索增强生成（RAG）系统进行自动化无参考评估的必要性。

具体而言，我们论证了构建一个评估框架的需求，该框架需能够对以下三个方面进行评估：
忠实度（即答案是否基于检索到的上下文）、
答案相关性（即答案是否回应了问题）
以及上下文相关性（即检索到的上下文是否具有足够的聚焦性）。

为支持该框架的开发，我们引入了 WikiEval 数据集，该数据集包含人类对上述三个不同方面的标注结果。

最后，我们还介绍了 Ragas—— 我们针对上述三个质量维度所实现的评估工具。
该框架易于使用，即便在缺乏任何真值标注的情况下，也能为检索增强生成系统的开发者提供有价值的参考信息。

我们在 WikiEval 数据集上的评估结果表明，Ragas 的预测结果与人类的判断高度吻合，在忠实度和答案相关性这两个维度上表现尤为突出。

3.评估策略
我们采用标准的检索增强生成（RAG）场景：

给定问题q，系统首先检索相关上下文c(q)，随后利用检索到的上下文生成答案as(q)。

在构建检索增强生成系统时，我们通常无法获取人工标注的数据集或参考答案。
因此，我们将研究重点放在完全自包含且无参考的评估指标上。
具体而言，我们聚焦于三个核心质量维度，我们认为这三个维度至关重要。

第一，忠实度（Faithfulness），指答案必须基于给定的上下文。
这一维度对于避免生成幻觉、确保检索到的上下文可作为生成答案的依据至关重要。
事实上，检索增强生成系统常被应用于对生成文本与参考来源的事实一致性要求极高的场景，
例如信息持续更新的法律领域。

第二，答案相关性（Answer Relevance），指生成的答案需准确回应所提出的问题。

第三，上下文相关性（Context Relevance），指检索到的上下文需具备聚焦性，尽可能减少无关信息。
考虑到向大语言模型（LLM）输入长上下文存在成本问题，
同时过长的上下文会降低大语言模型对其的利用效率 —— 尤其是对上下文中间部分信息的利用（Liu 等人，2023），这一维度尤为重要。

下文将阐述如何通过提示大语言模型，以完全自动化的方式对这三个质量维度进行度量。

在本研究的实现与实验过程中，所有提示均通过 OpenAI API 调用 gpt-3.5-turbo-16k 模型进行评估。

忠实度若答案中的所有主张均可从上下文中推断得出，则称答案as(q)对上下文c(q)具有忠实度。
为评估忠实度，我们首先利用大语言模型从答案中提取一组陈述S(as(q))。
此步骤的目的是将较长的句子分解为更简短、聚焦的断言。
该步骤使用的提示语如下：给定问题与答案，请从答案的每个句子中提炼一个或多个陈述。
问题：[问题内容]
答案：[答案内容]
其中，[问题内容] 和 [答案内容] 分别替换为具体的问题与答案。

对于集合S中的每个陈述s_i，大语言模型通过验证函数v(s_i, c(q))判断该陈述是否可从上下文c(q)中推断得出。
验证步骤使用的提示语如下：请结合给定上下文与下列陈述，判断这些陈述是否得到上下文中信息的支持。
在得出最终结论（是 / 否）前，请为每个陈述提供简要解释。

最后，请按顺序以指定格式给出每个陈述的最终结论，不得偏离该格式。
陈述：[陈述 1]
......
陈述：[陈述 n]

最终的忠实度得分F计算公式为：F=V/S，其中V为大语言模型判定为得到支持的陈述数量，S为陈述的总数量。

答案相关性
若答案以恰当的方式直接回应问题，则称答案a_s(q)具有相关性。
需要说明的是，我们对答案相关性的评估不考虑事实准确性，但会对答案存在遗漏或包含冗余信息的情况进行惩罚。
为评估答案相关性，针对给定答案a_s(q)，我们提示大语言模型基于该答案生成n个潜在问题q_i，使用的提示语如下：
请为给定答案生成一个对应的问题。
答案：[答案内容]

随后，我们通过 OpenAI API 调用 text-embedding-ada-002 模型，获取所有问题的嵌入向量。
对于每个潜在问题q_i，我们计算其与原始问题q的相似度sim(q, q_i)，即对应嵌入向量之间的余弦相似度。
问题q对应的答案相关性得分AR计算公式为：
计算每个潜在问题q_i和原始问题q的相似度，求和，再求平均值。

该指标用于评估生成答案与初始问题或指令的契合程度。

上下文相关性若上下文仅包含回答问题所需的信息，则称上下文c(q)具有相关性。
该指标旨在对包含冗余信息的情况进行惩罚。
为评估上下文相关性，给定问题q及其上下文c(q)，
我们提示大语言模型从上下文中提取对回答问题至关重要的句子子集S_ext，使用的提示语如下：

请从提供的上下文中提取可能有助于回答下列问题的相关句子。
若未找到相关句子，或你认为无法从给定上下文中回答该问题，请返回短语 “信息不足”。
提取候选句子时，不得对上下文中的句子进行任何修改。

上下文相关性得分的计算公式为：
CR=提取的句子数量/上下文中的总句子数量

4.WikiEval 数据集
为评估所提出的框架，我们理想情况下需要带有人类标注的 “问题 - 上下文 - 答案” 三元组示例，
进而验证我们的指标与人类对忠实度、答案相关性及上下文相关性评估结果的吻合程度。
由于目前尚无可用的公开数据集满足这一需求，我们构建了一个新的数据集，命名为 WikiEval⁴。
在数据集构建过程中，我们首先选取了 50 个维基百科页面，内容均为 2022 年初以来发生的事件⁵。
页面选择时，我们优先考虑近期经过编辑的条目。
针对这 50 个页面中的每一个，我们使用以下提示语，让 ChatGPT 基于页面的引言部分生成一个可被回答的问题：

你的任务是根据给定上下文，按照以下规则设计一个问题：
问题必须能从给定上下文中得到完整回答。
问题的设计需基于包含重要信息的内容片段。
答案中不得包含任何链接。
问题难度适中。
问题需表述合理，能够被人类理解并作答。
问题中不得使用 “给定上下文” 等类似表述。
上下文：

随后，我们将对应的引言部分作为上下文，使用以下提示语让 ChatGPT 回答生成的问题：
请利用给定上下文中的信息回答问题。
问题：[问题内容]
上下文：[上下文内容]

两位标注者针对上述三个核心质量维度，对所有问题进行了人工标注。
这两位标注者均精通英语，且已充分理解三个质量维度的定义。
在忠实度和上下文相关性维度上，两位标注者的一致率约为 95%；
在答案相关性维度上，一致率约为 90%。
标注分歧均通过标注者之间的讨论得到解决。

忠实度标注
为获取人类对忠实度的判断结果，我们首先让 ChatGPT 在无额外上下文的情况下回答问题。
随后，我们要求标注者结合问题及对应的维基百科页面，判断两个答案（即基于上下文的标准答案和无上下文生成的答案）中哪一个忠实度更高。

答案相关性标注
为获取答案相关性较低的候选答案，我们使用以下提示语让 ChatGPT 生成对应内容：
请以不完整的方式回答给定问题。问题：[问题内容]
随后，我们要求人类标注者对比该答案与标准答案，指出哪一个的答案相关性更高。

上下文相关性标注
为度量该维度，我们首先通过爬取对应维基百科页面的反向链接，为上下文补充额外句子。
通过这种方式，我们能够为上下文添加与主题相关但对回答问题帮助较小的信息。
对于少数无反向链接的页面，我们则使用 ChatGPT 对给定上下文进行扩展补充。

5.实验
表 1 分析了第 3 节提出的指标与 WikiEval 数据集人工评估结果之间的一致性。
每个 WikiEval 样本均要求模型对两个答案或两个上下文片段进行比较。
我们统计了模型偏好的答案 / 上下文（即预估忠实度、答案相关性或上下文相关性最高的答案 / 上下文）与人类标注者偏好的答案 / 上下文相吻合的次数，
并以准确率（即模型与标注者判断一致的样本占比）呈现结果。

为更直观地体现结果价值，我们将所提指标（表 1 中记为 Ragas）与两种基线方法进行对比。

第一种基线方法记为GPT 评分法（GPT Score），
我们让 ChatGPT 在 0-10 分范围内对三个质量维度进行打分。
具体操作时，我们会在提示语中先说明该质量指标的定义，再要求其根据定义对给定的答案 / 上下文进行评分。
例如，评估忠实度时使用的提示语如下：
忠实度用于衡量答案与给定上下文的信息一致性。答案中任何无法从上下文中推导得出的主张均应被扣分。
给定答案与上下文，请在 0-10 分范围内为忠实度打分。
上下文：[上下文内容]
答案：[答案内容]

若大语言模型为两个候选答案给出相同分数（即平局情况），则通过随机方式打破平局。

第二种基线方法记为GPT 排序法（GPT Ranking），该方法要求 ChatGPT 直接选出更优的答案 / 上下文。
此方法的提示语同样包含对应质量指标的定义。
例如，评估答案相关性时使用的提示语如下：
答案相关性用于衡量回答直接回应给定问题的程度以及与问题的适配程度。
对于包含冗余信息或未完整回答问题的情况，该指标会予以扣分。
给定问题与两个答案，请基于答案相关性对答案进行排序。
问题：[问题内容]
答案 1：[答案 1 内容]
答案 2：[答案 2 内容]

表 1 结果显示，与两种基线方法的预测结果相比，我们提出的指标与人类判断的吻合度要高得多。
在忠实度维度上，Ragas 的预测总体准确率很高；
在答案相关性维度上，两者的吻合度相对较低，这主要是因为两个候选答案之间的差异通常十分细微。
我们发现，上下文相关性是最难评估的质量维度。

具体而言，我们观察到 ChatGPT 在从上下文中筛选关键句子的任务上往往表现不佳，在处理较长上下文时尤为明显。

6.引言
语言模型（LMs）蕴含着海量的世界知识，使其能够在不访问任何外部资源的情况下回答问题。
自 BERT 模型问世后（德夫林等人，2019），语言模型作为知识存储库的理念便开始出现，
而随着更大规模语言模型的不断推出（罗伯茨等人，2020），这一理念得到了进一步确立。

尽管最新的大语言模型（LLMs）所掌握的知识，已使其在各类问答基准测试中具备与人类相当的表现（布贝克等人，2023），
但将大语言模型用作知识库仍存在两个根本性局限：

其一，大语言模型无法回答与其训练完成后发生的事件相关的问题；
其二，即便是规模最大的模型，也难以记住训练语料中极少提及的知识（坎德帕尔等人，2022；马伦等人，2023）。

针对这些问题，标准解决方案是采用检索增强生成（RAG）技术（李等人，2019；刘易斯等人，2020；顾等人，2020）。

该技术回答问题的核心流程为：
先从语料库中检索相关段落，再将这些段落与原始问题一同输入至语言模型。
早期方法依赖专用语言模型实现检索增强语言建模（坎德瓦尔等人，2020；博若等人，2022），
而近期研究表明，只需将检索到的文档添加至标准语言模型的输入中，也能取得良好效果（哈塔布等人，2022；拉姆等人，2023；史等人，2023）。
这使得检索增强策略可与仅通过应用程序接口（API）开放的大语言模型结合使用。

尽管检索增强策略的实用性已得到证实，但其落地实施需要大量调优工作 —— 检索模型、选用的语料库、语言模型、提示词设计等诸多因素，
都会影响系统的整体性能。
因此，对检索增强系统进行自动化评估至关重要。

在实际应用中，检索增强生成系统的评估往往围绕语言建模任务本身展开，即通过测量其在某一参考语料库上的困惑度来完成。

然而，此类评估结果并非总能预测系统在下游任务中的表现（王等人，2023c）。
此外，这种评估方式依赖于语言模型的概率输出，而部分闭源模型（如 ChatGPT 和 GPT-4）并不开放这一数据。

问答任务是另一种常用的评估方式，但该方式通常仅考虑包含简短抽取式答案的数据集，这可能无法准确反映系统的实际使用效果。

为解决上述问题，本文提出了Ragas框架，用于对检索增强生成系统进行自动化评估。
我们聚焦于无参考答案的场景，除评估检索段落的实用性外，还需估算多个可反映答案正确性的代理指标。
Ragas 框架与目前构建检索增强生成解决方案最常用的两个框架 ——llama-index 和 Langchain 实现了集成，便于开发者将其轻松融入标准工作流程。

7.相关工作
基于大语言模型的忠实度评估
大语言模型生成内容中的幻觉检测问题已得到广泛研究（季等人，2023）。
部分学者提出采用少样本提示策略来预测事实准确性（张等人，2023）。
但近期研究表明，现有模型在使用标准提示策略时，难以有效检测幻觉内容（李等人，2023；阿扎里亚、米切尔，2023）。
其他方法依赖于将生成内容与外部知识库中的事实进行关联（闵等人，2023），但这种方式并非在所有场景下都可行。

另一种策略是分析模型为单个标记分配的概率 —— 理论上，模型对幻觉答案的置信度应低于对事实性答案的置信度。
例如，BARTScore（袁等人，2021）通过计算给定输入下生成文本的条件概率来评估事实准确性。

卡达瓦思等人（2022）对这一思路进行了改进：基于大语言模型在回答多项选择题时能输出校准良好的概率这一观察，
他们将验证模型生成答案的问题转化为 “答案为真或为假” 的多项选择题。

与关注输出概率不同，阿扎里亚和米切尔（2023）提出在大语言模型某一隐藏层的权重上训练有监督分类器，
以预测给定陈述是否为真。该方法虽表现出色，但因需要访问模型的隐藏状态，无法应用于通过应用程序接口（API）调用大语言模型的系统。

对于 ChatGPT、GPT-4 等不开放标记概率的模型，需采用不同方法。
SelfCheckGPT（马纳库尔等人，2023）通过生成多个答案样本解决该问题，
其核心思想是：事实性答案具有更高的稳定性，不同样本在语义上更可能相似；而幻觉答案则难以保持这种语义一致性。

文本生成系统的自动化评估
除事实准确性外，大语言模型还被用于自动评估生成文本片段的其他维度。
例如，GPTScore（傅等人，2023）通过提示语明确评估维度（如流畅度），再基于给定自回归语言模型中生成标记的平均概率对文本进行评分。

袁等人（2021）此前也探讨过提示语的应用思路，但他们使用的是经微调的小型语言模型（即 BART），且未发现使用提示语能带来明显收益。

另一种方法是直接让 ChatGPT 对给定答案的特定维度进行评估，评分方式包括 0-100 分制或五星评级制（王等人，2023a）。
这种方式虽能取得优异结果，但存在对提示语设计高度敏感的局限性。

部分学者未对单个答案进行评分，而是聚焦于利用大语言模型从多个候选答案中选出最优答案（王等人，2023b），
这种方法通常用于对比不同大语言模型的性能。但需注意的是，答案的呈现顺序可能会影响评估结果（王等人，2023b）。

在现有研究中，真实答案或更通用的生成内容的使用方式上，大多数方法依赖于一个或多个参考答案的可用性。
例如，BERTScore（张等人，2020）和 MoverScore（赵等人，2019）利用预训练 BERT 模型生成的上下文嵌入，来比较生成答案与参考答案之间的相似度。
BARTScore（袁等人，2021）同样借助参考答案，计算精确率（定义为给定参考答案时生成该答案的概率）和召回率（定义为给定该答案时生成参考答案的概率）等指标。
