ARES: An Automated Evaluation Framework for Retrieval-Augmented
Generation Systems
https://arxiv.org/pdf/2311.09476
一、摘要
传统上，检索增强生成（RAG）系统的评估依赖于人工标注的输入查询、待检索段落以及待生成回复。

本文提出一种自动化 RAG 评估系统 ARES，用于从上下文相关性、回答忠实度和回答相关性三个维度对 RAG 系统进行评估。

ARES 通过生成自有合成训练数据，微调轻量级语言模型评判器，以评估 RAG 系统各组件的质量。

为减少潜在的预测误差，该系统采用少量人工标注数据点来执行预测驱动推理（PPI）。

在 KILT、SuperGLUE 和 AIS 基准的八项不同知识密集型任务中，ARES 仅需数百条人工标注数据即可实现对 RAG 系统的精准评估。

此外，该系统的评判器在领域迁移场景下仍保持有效性，即使被评估 RAG 系统的查询类型和 / 或文档类型发生变化，依然能实现准确评估。

相关代码与数据集已在 GitHub 上开源。https://github.com/stanford-futuredata/ARES


二、结论
本文提出了一种全新的检索增强生成（RAG）自动化评估框架 ——ARES。

该框架设计了一套新颖的训练流程，可基于合成生成的查询与回答对轻量级大语言模型评判器进行微调。

ARES 能够对 RAG 系统的各个组件分别展开评估，助力研发人员深化对系统的理解并制定针对性优化方案，且仅需极少的人工标注成本。

在 KILT、SuperGLUE 和 AIS 三大基准中需要基于 RAG 技术解决的八个不同数据集上的实验结果表明，
ARES 可依据上下文相关性、回答忠实度和回答相关性分数，对 RAG 系统进行精准评分与排序，性能超越现有自动化评估框架 RAGAS。

ARES 是一套具备高度灵活性的框架，其衍生变体的性能或可超越本文所探索的版本。
未来可探索的方向包括：采用 GPT-4 替代人工标注（见表 4）、
为微调大语言模型评判器设计更鲁棒的合成数据集构建技术、
利用大语言模型评判器预测过程中的对数几率提升预测驱动推理（PPI）置信区间的可靠性，
以及测试更先进的大语言模型作为 ARES 的微调评判器。


三、局限性
ARES 的运行依赖人工偏好验证集中的少量标注数据（约 150–300 个数据点，数据量越多效果越好）。

这类标注工作通常需要标注人员熟悉该 RAG 系统所属的领域应用场景。对于通用领域的应用，此类标注较易完成；但在法律、医学、金融等更专业的领域，可能需要具备专业知识的标注人员参与。

ARES 中使用的大语言模型（LLM）在很大程度上依赖具备大容量存储的显卡（GPU）硬件。

在本研究的 ARES 框架中，DeBERTa-v3-Large（3.04 亿参数）和 FLAN-T5-XXL（113 亿参数）模型需要运行在显存约 32GB 的显卡上，其微调与生成过程分别需要数小时。

尽管商用显卡目前普及度较高，但受成本限制，并非所有自然语言处理研究人员和技术人员都能轻松获取。

此外，本研究用于评估 ARES 的所有数据集均为英文数据集 —— 英文作为资源丰富的语言，拥有大量标注数据。

未来研究应探索如何通过为 ARES 的评判器和合成数据生成模块选用不同的大语言模型，将 ARES 拓展至其他语言场景。这将有助于我们更全面地认识现有 ARES 框架的优势与不足。

四、方法

ARES 系统的执行流程分为三个阶段（见图 1），
其运行需要三项必要输入：领域内段落集、包含约 150 条及以上标注数据点的人工偏好验证集，以及少量领域内查询 - 回答示例（5 条及以上）—— 这些示例用于在合成数据生成阶段提示大语言模型。
输入数据准备完成后，首先基于目标语料库中的段落生成合成查询语句（及其对应回答）；
随后，利用这些 “查询 - 段落 - 回答” 三元组训练大语言模型评判器；
最后，将训练好的评判器应用于待评估的 RAG 系统，对其生成的领域内 “查询 - 文档 - 回答” 三元组样本进行评分，并结合人工偏好验证集，通过预测驱动推理（PPI）方法估算每个 RAG 系统性能的置信区间。

4.1基于大语言模型的合成数据集生成
本研究采用生成式大语言模型，基于语料库段落生成合成查询语句与对应回答。
生成的数据集包含 “查询 - 段落 - 回答” 三元组的正例与反例（例如，相关 / 不相关的段落、正确 / 错误的回答）。
生成过程中，大语言模型以输入的少量领域内示例为参考 —— 这些示例实现了领域内段落与领域内查询、回答的映射；
随后，模型基于给定的领域内段落生成合成问题与回答，从而构建正、负两类训练样本。

合成数据生成的核心模型为 FLAN-T5 XXL（详见 4.1 节）。
实验表明，该模型与 ARES 系统的适配性良好（见第 5 章），但从理论上，系统也可兼容其他高性能模型完成合成查询与回答的生成任务。
生成原始合成查询后，需进行质量过滤：
将查询语句输入检索器，若原始段落能被检索为最优结果，则保留该查询，反之则剔除。
该过滤方法已在多项前期研究中被证实可有效筛选高质量合成查询（戴等人，2022；萨德 - 法尔孔等人，2023）。

为构建大语言模型评判器微调所需的负例，本研究提出两种全新策略，且两种策略生成的负例数量相同：
弱负例生成：针对上下文相关性负例，随机抽取与给定合成查询无关的领域内段落；针对回答忠实度与回答相关性负例，随机抽取由 FLAN-T5 XXL 基于其他段落生成的合成回答。

强负例生成：针对上下文相关性负例，从正例段落所属的同一文档中随机抽取其他段落；
若数据集不支持从同一文档获取多段文本，则采用 BM25 算法检索与正例段落最相似的前 10 段文本，并从中抽样作为上下文相关性强负例。
针对回答忠实度与回答相关性负例，利用附录 A.5 中的少样本提示词，引导 FLAN-T5 XXL（详见 4.1 节）生成与正例回答相矛盾的内容。

总体而言，用于上下文相关性与回答相关性评估的负例数量，与对应任务的正例数量保持一致。

4.2大语言模型评判器的构建
为实现 RAG 系统的评估，本研究利用合成数据集对 DeBERTa-v3-Large 模型进行微调，构建大语言模型评判器（详见 4.1 节），使其具备以下三项评估能力（陈等人，2023；詹姆斯、埃斯，2023）：
上下文相关性：检索返回的段落是否与回答给定查询相关？
回答忠实度：生成的回答是否忠实于检索到的段落，是否包含超出段落内容的幻觉信息或推断内容？
回答相关性：结合查询语句与检索段落，生成的回答是否具有相关性？
针对上述每项评估指标，本研究均构建独立的大语言模型，并在模型顶部接入二分类器头，通过微调实现正、负例的分类。
对于每条拼接后的 “查询 - 文档 - 回答” 数据，单个大语言模型评判器需依据其对应的评估指标，将该三元组划分为正例或负例。
微调过程中，利用人工偏好验证集监控每一轮训练后的模型性能提升情况，若连续三轮训练损失未下降，则停止微调（更多细节见附录 A.1）。

4.3基于置信区间的 RAG 系统排序
大语言模型评判器构建完成后，需利用其对候选 RAG 系统进行评分与排序。

具体流程为：ARES 系统从各 RAG 系统生成的领域内 “查询 - 文档 - 回答” 三元组中抽样，由评判器对每条样本进行标注，预测其上下文相关性、回答忠实度与回答相关性。

通过计算所有领域内样本预测标签的平均值，得到各 RAG 系统在三项指标上的性能得分。

理论上，可直接将上述平均得分作为各 RAG 系统的质量指标。

但需注意，这些得分基于未标注数据与合成训练的大语言模型评判器预测结果，其准确性无法得到完全保证。

另一种极端方案是仅利用前文提及的小型人工偏好验证集进行评估，通过对比各 RAG 系统与人工标注的一致性（或偏差程度）完成评价。

然而，这种基于标注的评估方法需要为每个 RAG 系统单独标注大量生成结果，时间与资金成本均较高。

为结合两种方法的优势、提升评估精度，ARES 系统采用预测驱动推理（PPI；安杰洛普洛斯等人，2023）方法预测系统得分。

预测驱动推理是一种最新的统计方法，通过结合小批量标注数据（即本研究的验证集）与大量未标注数据的预测结果，能够生成更紧凑的置信区间。

该方法可同时利用标注数据与大语言模型评判器对未标注数据的预测结果，为 RAG 系统的性能构建置信区间。

具体实现时，预测驱动推理首先利用大语言模型评判器处理人工偏好验证集，学习一个校正函数；

随后，结合大量未标注数据中的模型预测结果，构建机器学习模型性能的置信集。

与仅使用 RAG 系统标注结果的方法相比，该置信集能够为待评估 RAG 系统的性能（如上下文相关性、回答忠实度或回答相关性的单独准确率）生成更紧凑的置信区间。

通过将人工偏好验证集与大量带机器学习预测结果的数据相结合，预测驱动推理可生成比传统推理方法更可靠的机器学习模型性能置信区间。

借助预测驱动推理的校正函数，可估算大语言模型评判器的误差，并为 RAG 系统的成功率与失败率生成置信边界，从而实现对上下文相关性、回答忠实度与回答相关性性能的评估。

此外，预测驱动推理支持按选定的概率水平估算置信区间；在本研究的实验中，采用标准的 95% 置信水平（概率）。

得到 RAG 系统各组件的准确率置信区间后，取每个置信区间的中点，并基于这些中点值对 RAG 系统进行排序。

通过该排序结果，可对比不同的 RAG 系统，或同一 RAG 系统的不同配置，从而确定特定领域下的最优方案。


eg：
我们以医疗领域的患者咨询 RAG 系统为具体场景，详细拆解 ARES 系统的三大核心流程（合成数据集生成→LLM 评判器构建→基于置信区间的 RAG 排序）。

该场景中，待评估的 RAG 系统需从医疗指南语料库中检索信息，回答患者关于 “糖尿病饮食注意事项” 的咨询，ARES 的目标是评估其上下文相关性、回答忠实度和回答相关性。

前置准备：确定 ARES 的三项输入

在启动流程前，需准备 ARES 要求的三类输入：

1.领域内段落集：选取《中国 2 型糖尿病防治指南》中的饮食相关章节，提取 1000 段专业文本（如 “糖尿病患者应控制碳水化合物摄入，每日主食量不超过 250g”）。
2.人工偏好验证集：标注 150 条 “查询 - 段落 - 回答” 三元组，覆盖正例与反例。例如：
正例：查询 “糖尿病能吃米饭吗？”→段落 “米饭属于碳水化合物，每日摄入量建议控制在 100-150g”→回答 “糖尿病患者可以吃米饭，每日不超过 150g”。
反例：查询 “糖尿病能吃米饭吗？”→段落 “糖尿病患者应减少高糖水果摄入”→回答 “糖尿病患者不能吃米饭”。
3.少样本示例：准备 5 条医疗领域的查询 - 回答示例（如 “查询：糖尿病能吃苹果吗？→回答：可以，每日 1 个中等大小苹果”），用于合成数据生成的提示。

流程 1：LLM 生成合成数据集
本阶段的目标是基于医疗指南段落，生成大量带正 / 负例的 “查询 - 段落 - 回答” 三元组，为训练评判器提供数据。

步骤 1.1：生成原始合成查询与回答
选用 FLAN-T5 XXL 作为生成模型，输入少样本示例和医疗指南段落，引导模型生成合成数据。
输入段落：“糖尿病患者应避免食用精制糖，如白糖、红糖，可选择代糖如甜菊糖。”
模型生成查询：“糖尿病患者能吃红糖吗？”
模型生成回答：“糖尿病患者不能吃红糖，建议选择甜菊糖等代糖。”

步骤 1.2：过滤低质量查询
将生成的查询输入待评估 RAG 系统的检索器，验证是否能检索到原始段落。若检索结果的 Top1 是原始段落，则保留该查询；若检索到无关段落（如 “糖尿病运动注意事项”），则剔除该查询。
最终筛选出 800 条高质量原始三元组（正例）。

步骤 1.3：生成负例（弱负例 + 强负例）
为了让评判器学会区分 “好 / 坏” 结果，需生成与正例数量相等的负例（800 条），采用 ARES 的两种策略：

弱负例生成
上下文相关性弱负例：为查询 “糖尿病能吃红糖吗？” 随机匹配无关段落（如 “糖尿病患者应定期监测血糖”）。
回答忠实度 / 相关性弱负例：为查询 “糖尿病能吃红糖吗？” 匹配正确段落，但随机替换为其他查询的回答（如 “可以吃，每日不超过 50g”）。

强负例生成（难度更高，更贴近真实错误）
上下文相关性强负例：为查询 “糖尿病能吃红糖吗？” 的正例段落（“避免精制糖，如白糖、红糖”），从同一文档中抽取相邻段落（“糖尿病患者可适量食用全谷物”），该段落与查询相关但非最优。
回答忠实度 / 相关性强负例：提示 FLAN-T5 XXL 生成与正例矛盾的回答，如 “糖尿病患者可以吃红糖，对血糖影响不大”。

最终，合成数据集包含 800 条正例 + 800 条负例，共 1600 条三元组，覆盖上下文相关性、回答忠实度、回答相关性三个维度。

流程 2：准备 LLM 评判器
本阶段的目标是训练三个独立的评判器，分别评估 RAG 系统的三个核心指标。选用 DeBERTa-v3-Large 作为基础模型，为每个指标添加二分类器头（输出 “正例 / 负例”）。

步骤 2.1：定义评判器任务
上下文相关性评判器：输入 “查询 + 段落”，判断段落是否能回答查询（标签：相关 / 不相关）。
回答忠实度评判器：输入 “段落 + 回答”，判断回答是否忠实于段落（无幻觉 / 有幻觉）。
回答相关性评判器：输入 “查询 + 段落 + 回答”，判断回答是否针对查询（相关 / 不相关）。

步骤 2.2：微调评判器
用合成数据集对三个评判器分别进行微调，训练过程中利用人工偏好验证集监控性能：
每轮训练后，用验证集计算评判器的分类准确率。
若连续 3 轮训练损失未下降，则停止微调（防止过拟合）。
例如，上下文相关性评判器在微调后，能准确区分 “查询 - 段落” 对是否相关：
对于 “糖尿病能吃红糖吗？+ 避免精制糖如白糖、红糖”，输出 “相关”；
对于 “糖尿病能吃红糖吗？+ 定期监测血糖”，输出 “不相关”。

流程 3： Ranking RAG Systems with Confidence Intervals
本阶段的目标是用训练好的评判器评估待评估的 RAG 系统，并结合 PPI 方法生成置信区间，最终完成排序。

假设待评估的有两个 RAG 系统：RAG-A（使用 BM25 检索器）和RAG-B（使用向量检索器）。

步骤 3.1：对 RAG 系统生成的三元组评分
从 RAG-A 和 RAG-B 中各抽样 200 条 “查询 - 段落 - 回答” 三元组（由系统处理真实患者查询生成），用三个评判器分别评分：
每个三元组的每个指标得分为 0（负例）或 1（正例）。
计算平均值，得到初步得分。例如：
RAG-A：上下文相关性 0.75，回答忠实度 0.68，回答相关性 0.72。
RAG-B：上下文相关性 0.82，回答忠实度 0.78，回答相关性 0.80。

步骤 3.2：用 PPI 方法优化得分并生成置信区间
初步得分仅基于评判器预测，可能存在误差。ARES 引入 PPI 方法，结合人工偏好验证集（150 条标注数据）和大量未标注数据（200 条抽样数据），生成更可靠的置信区间：
训练校正函数：用评判器对验证集进行预测，对比人工标注结果，学习评判器的误差规律（如评判器对 “代糖” 相关内容的预测误差较高）。
生成置信区间：结合校正函数和未标注数据的预测结果，为每个指标生成 95% 置信区间。例如：
RAG-A 的回答忠实度：0.68 ± 0.05（置信区间：[0.63, 0.73]）。
RAG-B 的回答忠实度：0.78 ± 0.03（置信区间：[0.75, 0.81]）。

步骤 3.3：基于置信区间中点排序
取每个指标置信区间的中点，计算综合得分（如三个指标的平均值），最终排序：
RAG-A 综合得分：(0.75 + 0.68 + 0.72)/3 = 0.717。
RAG-B 综合得分：(0.82 + 0.78 + 0.80)/3 = 0.800。
结论：RAG-B 的性能优于 RAG-A，更适合医疗领域的糖尿病饮食咨询场景。

总结
通过医疗领域的实例，我们可以看到 ARES 的核心优势：
1.无需大量人工标注，仅用 150 条验证集即可完成评估。

2.能针对 RAG 系统的三个核心组件分别评估，定位性能瓶颈（如 RAG-A 的回答忠实度较低，可能是因为检索器返回的段落不够精准）。

3.结合 PPI 方法生成置信区间，评估结果更可靠，避免因随机抽样导致的误差。


5 实验
5.1 模型
在微调评判器方面，ARES 借助大语言模型生成成本低廉且质量可靠的合成查询与回答。

合成数据集的生成采用 FLAN-T5 XXL 模型（钟等人，2022），微调后的大语言模型评判器则选用 DeBERTa-v3-Large 模型（何等人，2021）。

经微调的大语言模型评判器使我们能够在不依赖外部应用程序编程接口（API）的情况下对 RAG 系统进行排序，仅需借助少样本提示词，并在商用图形处理器（GPU）上部署大语言模型即可完成任务。

在上下文学习基线模型方面，我们采用 OpenAI 公司的 GPT-3.5-turbo-16k（2023 年 10 月版本）（布朗等人，2020），并设置为零样本 / 少样本学习模式。

针对领域内段落的相似度检索，我们使用 FAISS 库中的 IndexFlatL2 算法进行索引构建（约翰逊等人，2019），同时采用 OpenAI 公司的 text-embedding-ada-002 模型生成嵌入向量。

我们通过对领域内段落执行相似度检索，过滤掉那些无法检索到其生成来源段落的合成查询。实验中使用的 RAGAS 框架版本为 0.0.18（詹姆斯、埃斯，2023）。

5.2 数据集
本研究的核心实验目标是全面展示 ARES 的有效应用场景。

为了在多种查询类型、文档类型和回答类型下开展测试，我们从被广泛使用的 KILT 和 SuperGLUE 基准中，选取了所有适用于 RAG 技术的数据集。

在 KILT 基准（佩特罗尼等人，2021）中，我们选用了自然问题数据集（NQ）、热点问答数据集（HotpotQA）、事实提取与验证数据集（FEVER）以及
维基百科对话数据集（WoW）（克维亚特科夫斯基等人，2019；杨等人，2018；阿赫塔尔等人，2023；迪南等人，2018）。

这些数据集均以维基百科段落为基础，但查询与回答涵盖了多种应用场景。自然问题数据集和热点问答数据集的特点是包含直接性问题，且要求生成简短回答；

不同之处在于，自然问题数据集仅需单段文本即可完成推理，而热点问答数据集则需要多段文本进行推理。

此外，事实提取与验证数据集聚焦于事实验证任务，需判断某一段落对给定陈述是支持还是反驳，输出结果为 “支持” 或 “反驳”。

维基百科对话数据集旨在评估对话代理的性能，其流程为：先将用户对话与相关的维基百科段落进行匹配，再由聊天机器人结合段落中的知识生成段落长度的对话回复。

在 SuperGLUE 基准（王等人，2019）中，我们选用了多段落阅读理解数据集（MultiRC）和阅读理解与记录数据集（ReCoRD）（哈沙比等人，2018；张等人，2018）。

多段落阅读理解数据集包含七个不同领域的直接性问题，涉及的领域包括新闻、维基百科文章、社会 / 法律 / 司法类文章、历史 / 人类学类文章、小学科学教材、“9・11” 事件调查报告以及虚构作品。

阅读理解与记录数据集的任务是确定陈述中的占位实体，其数据来源为美国有线电视新闻网（CNN）和《每日邮报》的新闻文章。

针对多段落阅读理解数据集和阅读理解与记录数据集，我们构建了其开放领域版本的任务。

对于多段落阅读理解数据集，我们在其七个领域的段落集上执行检索操作；

对于阅读理解与记录数据集，我们则在其新闻文章段落集上进行检索。

ARES 的有效性体现在，它仅需借助人工偏好验证集和面向特定领域的大语言模型评判器，就能对不同的 RAG 系统进行排序。

为了测试 ARES 的性能极限，我们需要模拟多个 RAG 系统的存在，且这些系统在评估指标上的准确率差异较小。

为此，我们利用人工构建的 “查询 - 段落 - 回答” 三元组创建了多个测试系统，通过这种方式，我们可以凭经验确定模拟 RAG 系统的正例和反例。

我们对给定的数据集进行模拟拆分，生成测试用的模拟系统，具体方式为：
（1）针对上下文相关性，筛选出正例和反例的 “查询 - 段落” 匹配对；
（2）针对回答相关性，筛选出正例和反例的 “查询 - 段落 - 回答” 匹配对。评估集中的正例和反例示例详见表 7。

对于正例三元组，我们直接使用 KILT 和 SuperGLUE 基准中的示例，不做任何修改。

在收集反例 “查询 - 段落” 对和反例 “查询 - 段落 - 回答” 三元组时，我们从以下两种来源中随机抽取段落和回答：同一篇维基百科文档，或完全随机的另一篇维基百科文档。
通过这种抽样方式，我们能够人工构建用于测试 ARES 的模拟 RAG 系统。
我们希望通过对相关和不相关的文档 / 回答进行抽样，更准确地评估 ARES 在评判 RAG 系统输出结果时的有效性。
由于缺乏人工标注的包含幻觉信息的回答用于评估，我们未在 KILT 和 SuperGLUE 数据集上对回答忠实度进行评估。不过，在 5.2 节中，我们将在真实的归因数据集上对 ARES 框架进行测试。
利用 KILT 和 SuperGLUE 各数据集的验证子集，我们构建了九种不同的数据集拆分版本。
对于每项评估的 RAG 指标，这些拆分版本的成功率范围从 70% 到 90% 不等，相邻版本之间的准确率相差 2.5 个百分点（例如 70.0%、72.5%、75.0%……90.0%）。

每种拆分版本对应一个不同的模拟 RAG 系统。

由于我们已知每个数据集拆分版本的成功百分比，因此也明确各模拟 RAG 系统应有的排序。

这使我们能够测试 ARES 在三项评估指标上，对模拟 RAG 系统进行准确评分和排序的能力。

5.3 评估指标
为了计算正确排序与 ARES 排序之间的相关性，我们采用肯德尔等级相关系数（简称肯德尔系数 τ），其计算公式如下：τ =（和谐对数量 − 不和谐对数量）/ 总对数

和谐对的定义是：在排序中，序列中靠前的序数小于靠后的序数。
不和谐对的定义是：在排序中，序列中靠前的序数大于或等于靠后的序数。
肯德尔系数 τ 的取值范围在 0.0 到 1.0 之间，当该系数大于 0.9 时，认为排序结果是成功的。

在研发过程中，研究人员和工程师会通过对模型选择、检索器选择以及文档预处理等方面进行逐一的成对比较，来对比不同的 RAG 配置。
我们需要确保，当 RAG 系统之间存在不同程度的性能差距时，ARES 在成对比较中仍能保持令人满意的准确率。
肯德尔系数 τ 是专门用于衡量此类成对比较准确率的指标，可计算完全准确的成对排序与实验得到的成对排序之间的相关性。
因此，它是信息检索领域中一种常用且应用广泛的指标，能够帮助开发人员通过实证方式评估排序系统。
基于此，我们认为肯德尔系数 τ 和预测准确率是测试 ARES 作为 RAG 评估系统有效性的有意义指标。


